{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuOULTAlprRc"
   },
   "source": [
    "The goal of this project is to determine whether face-selective neurons in TopoNets are necessary and sufficient for accurate face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAJDsEFapPFh",
    "outputId": "c699d8a1-7056-44d2-c578-7e12c65455ca"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install git+https://github.com/toponets/toponets.git #download pretrained toponets checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLqFgF2hpqQh",
    "outputId": "501c4a95-43d4-4781-905b-6df4dfc8b092"
   },
   "outputs": [],
   "source": [
    "import toponets\n",
    "topo_resnet18 = toponets.resnet18(tau=10.0, checkpoint_path = f\"resnet18_tau_{10}.pt\")\n",
    "topo_resnet50 = toponets.resnet50(tau=30.0, checkpoint_path = f\"resnet50_tau_{30}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vck9h3w4rEur",
    "outputId": "8b16c9c6-7ac7-426c-81a8-8397f95ff309"
   },
   "outputs": [],
   "source": [
    "!pip install datasets huggingface_hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "77dc7b3a66f1446e8dff3d0de6e4bd84",
      "27be5ccb9fea4e85948e9165ba4c13e6",
      "fdfebedcdf064094ae24b12084c1e865",
      "ace677bb682740728a824a301e1daf4d",
      "661b20f2ab49488ca9469f87c5268b80",
      "e589eba8792f43c88a33adf7adb31b6f",
      "3484e727048c44a4a6afdb42deb0371b",
      "9d6e4dfc651d4e02832e3ba36475ee4c",
      "d836cf7182a24c29bf6696499edcc291",
      "0717afbd52484b9da8525bf888dd583a",
      "d0f9354af56d4a5d841455b3c5920647",
      "ec77734b5b604c66a4ae41b23888b9f3",
      "6282282e96f44508aa0244592e3da123",
      "72f7b7dce05447df969cc6eb7fdaf50f",
      "2e53d11299da4af187d6705077e7bc6e",
      "5e2fd4acac4349639da9fdd02b9054b1",
      "8dc0ad2bfe7049f598258f3a0b86cd97",
      "ce1e909d927e4c7a919987618203bfba",
      "f3f0cf0b256049598a6a43ac0d8b29f0",
      "528bc40f517f4cf59512a4b09b2cc70d",
      "4571d0f21f2c4615a94df40665896586",
      "781b6601033c4a1fb97d1381a41e6664",
      "b32593aaf5944c9a9ee9dc413ba9e900",
      "170e88f5ba8a4001b7cfcc9ec9a98d04",
      "78de3dcad4db4f6a9b7ac3a81e96cc2e",
      "387e9063cbd34760a40c88fb52c9717f",
      "36ecebebc41e4679811e6c5ac7a37589",
      "d1bf476666e643c79b3cb0974fdd083f",
      "522f802dfe90434498f0a4e4d005a343",
      "fb3595ee36574c3287e3136f1bde659d",
      "140b8095ec264796b8976e077ef71375",
      "db71ad531acb45a6b44216d570fb921f",
      "e44e3066e2a94a13a75ca0f9968390dd",
      "f1a41a7031064c91a61cb8a2da1ed53d",
      "48e560f686634669a343863270ae7a3f",
      "b7404342109145b396b6d0764afd2c91",
      "22adfada010441c898b77ae55a227a95",
      "816972f5df4c479393d375a21955344b",
      "023ac3b631174a90bb8374b061c0614b",
      "6f4d95e9d7cb45319d51fae120cbb5dc",
      "b2730f77554142a4926edd3b1f92eaa9",
      "6e273b173964427291fbe37631684733",
      "a829f10f1bd34c74a526857374ba0752",
      "13ecbc977966447ba3cec512ae6e1f7f",
      "4b00a28089b64c8a8f177c74d0bc72ee",
      "87de4fcca4124734bc305326cbb94b86",
      "3778799f98554c059c1a662e3c9131d2",
      "defb5f2db87c45b8a704bf9df41fc5af",
      "d5a3bc08f4fa401ebab69dc43eb29ab3",
      "bc612eb28f2d48698709a7fe83a1c13a",
      "802ea4c3ca9f412d97d60c4191afc0e9",
      "88e549926e1647a3b1070f4be5ae87b6",
      "365e48b7c7b44fdebde39e2641ab0258",
      "904f4a546be64925a0554874ec4d6c3b",
      "b4a8c8af9b2440a7b8d77f8003dfebee",
      "fbb279c17d7f44a88a526c6a0793125d",
      "f527d6d448ab4664af1ec742ae5efe4a",
      "a46b683d4ed944cb82b85d054746759a",
      "9f03f24535334f06802861ce66736ee9",
      "5fe7e3f59a8344c8abf89751271aa59e",
      "1b276be7095c4980a96ac7646d788233",
      "417339996d04484e87fd528b0662fe49",
      "1a350b95e9544414ad91ce084adcee92",
      "e4e5784c22ec47f6b0025269f791e83e",
      "55c20737acf844d9853ab0dbc51d59cb",
      "c53b7daa89544c8c8c7444b8084b52d1",
      "2f8a05cbe4aa401ebca2bc65c3a17cf4",
      "5eed5e13176b4b2c9e11a19296be2e07",
      "94cb330e8d3943568cafbc9f03087e5f",
      "6ae28c3d13d3421a8b5c3bf2b90dc99d",
      "ff3609766dcf4dfc8f864313a798f332",
      "0ec9810bf7d84830a43a61967489fe04",
      "151333a077e544afab009a95518cd2bf",
      "0cccd77d352d429c99803d6c2c69ba35",
      "a5f63e072bf2457089f6ed5d6091591b",
      "e9e2e14fecf1424f86d4986a5bb369a1",
      "e1b4f4fb904b41f898e01389a14b1db8",
      "fac3d5c502a64776bbe9d7cdf1277859",
      "e3efae5a7bb14d2ea49355c5a058b63e",
      "c9a39c31cfcd4240a69f8bba31640f47",
      "cf9593fab3b4496292f6be8b5149590f",
      "51c66c5f40ff4d8d87630e5b3b017eb0",
      "d2cd500d30f44ff1afcd9130a8c992c9",
      "e8cc1951e7374169a22b456adf46672d",
      "441840308f644d96913b5a48b5ddb1d6",
      "c0b9c97b80e4430583e2ce4938bffc31",
      "f08029e41f354f34b8daa6da4bc2028a",
      "243614cee4e84f1ebc743ecd3b98fea4",
      "3d5280fc79e34c30b1ed70a9f7d3c60b",
      "34ff5bd9756447cbb618a89a4c58bb57",
      "79108bfed5d5446e8329c5b3ca027bc8",
      "4c272208ffb44f2c8424cfe2432975b8",
      "0e9ec28292654b12ba382f020fa76458",
      "d624dcb331a54b69a4b41a9ea3a7f2ff",
      "86608c67eabd4dabbea4abe3aadf4a0a",
      "6c01b1126a9845f8b62c4f4929f91d87",
      "021282b6364f413fa8582f818cd94ebe",
      "7dc906eb09fd44b4a49c68ef0001a4f5",
      "44a36aae963645888fc8d58eca3b1ae9",
      "b5d23cfe917842d48bca95f9b1009134",
      "102a3295af3f4d229c3bde91304b2444",
      "8c6e01f678d546a7b20ef8cfab85890e",
      "41d5aaa5a198443fbf1a45fc3396973d",
      "682079bc9c3d432aad2566a965c79d78",
      "236754c8ef494d628929ba1743ef16aa",
      "77052188ef9145f69af300e157035ef3",
      "6d70234bddf149198d307eaae65bccb0",
      "0cad3127864c4defadd5e8fe2bcaa40d",
      "3a5b75e5dc01453fb5a07cb391791c75",
      "158515c1a8f7468fa9ffa7e826231718",
      "9664f33523f5474da73c7096010536df",
      "b30726350d2c45f6bb7f45a300ca272e",
      "5d99edf5f9294d5ea373ac778135d979",
      "6624158b929f461f82b8a2ee424f4f74",
      "ac78aa2056d34d1cb0f26110bd054ff4",
      "a025973e2fec48d48a60ba8bf14618ed",
      "112310e6f845497487fa7986cacc563d",
      "4e6662a0ebf9428ebaa568e664b82391",
      "6bc35bc3a3654cd89d3edf344ea65dce",
      "675a3794d02a43fab1612ca94d13d142",
      "d629344565534f6d85b13ffead79212b",
      "b5521d684a404903aec8d1522fed5ff3",
      "598a1a8b4116496a8c61a26bef5ad9c0",
      "86a2729bc4434a719252a54148242526",
      "939541dbcce248008df875642ed10bd7",
      "5b9797de764f419b94403cf62071802f",
      "3fc5bc3e686d487eb43a40273fb4cf7d",
      "79b755b3157e404187e47ff2281b9734",
      "dde2fe961c014bde8a8856026ef33447",
      "077a5a642db646a2bab1728be33f386c",
      "9027c17e24f146d5ab83d732aed78669",
      "238300d1498d40ab8fbeaafff5998fde",
      "f6d54451d99b4885822d43ad691799d7",
      "7f61f46367bc4aeeac87092e92e007ac",
      "488adf7da0304679a77599b5f5145b82",
      "3c67e97de33f44208f5087b32fab7ed5",
      "5cec8efc461b4b678150e3ad84b0654f",
      "fe954d724d8f4bdea130354c7db20d59",
      "0f8319611d794d77a24d9ceca9c0c38a",
      "2c3bb1fc7c434ad7b429a7c6addf297e",
      "bc810529bf0642408dbed5d2062d2187",
      "9f44c70349044dee865c8b2c5aaedc23",
      "79760560fbbd4f3d84fea4ea6ebdb1df",
      "b3227e1eb0a7420eb07d667acab51782",
      "063190ca0d3e4be3af103548e1743cb8",
      "c053239af6cd4e1ebbef9044291f1720",
      "044f6149855540dcbc48b5827efd79e9",
      "aeaa1de159dd4891af76e8f656b6649e",
      "78205ffeda0445909ba4529d95ab2985",
      "5f0a1c75426448d3b15579047be678ef",
      "cd43ddc6103a49418cae99a3f84a7982",
      "31350c31233e4586be5ef80934a2c586",
      "85064f1cd70142b58a54278ca3b91977",
      "5cda262bff8c465ea0bd61791baa11ff",
      "2e4760b89eaa4b0d9e1b64a59f8767e1",
      "0127d3fefcfd4c7e96cb043adbded244",
      "8c6016545aa3413594041daf17e39773",
      "67a3de8a14954a0a817ec1c189259ca8",
      "feb6e0bf1a6744879b66fb5292205b34",
      "2cb5e4697e8d4094a3673eb014e4a065",
      "60052e5441b449d4b0d7554d2230db8b",
      "80bee760a5104d5891e5f9dbf6cb3a3b",
      "395413de123c44a597826d2d23eaa37e",
      "897b3d3324e3489bb19a4e02988fb2ac",
      "684a5b08de314080b54c2d54e9e6fc9b",
      "f1669cf4749a4174ae0d837ef47ac844",
      "d0640f90f1684825acf2ca039cf6bf5d",
      "d09280e701484ed29bfde7fcd22726e5",
      "ba03963d07a344518ce83d26d7345118",
      "8c260aac65da427490c0eb6cc364371d",
      "c92d19b489b040bebb1a075b526d05a3",
      "bbc2eb4801994de7ad87145e6408c6d6",
      "f1c0d42d91b6487ab1b8a4d860520b2f",
      "3dba411ba7024155aa15f8c1eaabf96e",
      "62cee0b058d04bc78a3f413db84d063f",
      "bc9db1a697054ddf8cbe9ea56928cd59",
      "8717bc4519294319b00c504c09b2cbf4",
      "4f8755b2877344518401f48c4dda6005",
      "061d5017827c4ab893fecd771f50d200",
      "49d267d24cdd433c94ddfbc0e1e2daba",
      "194904f0c524484eab6a3a6cdeda2e85",
      "4aa39dca188749e3bb0b9c545282c111",
      "0c0384945bed4b0884775c4bedd1d3dc",
      "a6274e80c7de425d992ef4ed1ed1ee2e",
      "57051639597f4904b311699d6a94ece8",
      "faff4ecded7247fbbb21451e26c72667",
      "e7c03344f4394c16856d90815a1874ca",
      "96543efa0d174b16b0619240fee55605",
      "8f807d60770747b698daa29b0bde5a80",
      "1ef5ba9ac7c142b2b6db107936649e4a",
      "f2f21559a9a44e6b8ddb63558ee5263f",
      "6a930ecc46bb468e82d93792c5cc5d76",
      "bb6ec654a44d483a895988667cb6af9c",
      "ad7f421a38ae4716bafa1425a5b44031",
      "cf4b92082568490aa65efe757fba4587",
      "12f40db736484c5fa087433f1020b148",
      "ad626742437d43d0bae89a403dda646b",
      "4ee2afba6ff047d18f43af4d0da9bbb7",
      "cf12d90861154416b6140d62c548e379",
      "a8aaec7be340469e86079cd3f6f43a30",
      "0bf4fa058af74055b74037af637c7a0b",
      "5ba7eb86277342c6b40f0db93753e5d0",
      "bc759c050e594ae1a160165db203abfd",
      "4e6e14859651466585389428d77e4da7",
      "27444e6b2f0e4f72a7678b3e02c36aa7",
      "39362295eb4b49fbac2fe8c8312e247b",
      "b1b31cad66454b18ab7c318defaafde9",
      "ba9bfab0e53e4a0c9afe27aa6f4dedfe",
      "4b196fe74ecb452f9c47bdd0db9846d7",
      "bd48d975abe84715bb50ec8386c35d35",
      "304c83b7579f4998a62c15777a63f8dd",
      "9bf30b08bbac4586a61da87c0980cd5e",
      "8041b15fe2ff48c3b131695866be7fb9",
      "6f671f63bc8643d88a5796b6ffff877d",
      "260e27181453411b8e78ff75a68c24a9",
      "6aea18c82d4144b289378c737d01ccbc",
      "d750d4b95ec5433b9202cb0333e71b39",
      "9dcbd55d6a2044089a81800305fba3ea",
      "7b50dec55da74c81aa812c386be75ba6",
      "79a863bc7e8a4ed598da8fe1f21ad570",
      "f711ead35ab54d858357331894c1f205",
      "bd02de487f4b472e87605f2b423314d9",
      "275cc246f90e490f83bd0ef7c33cd6b7",
      "21cec8232664463d828e311e0aa75410",
      "7f2c8bb60a984f2f9b4b9c2ee46dcc29",
      "7e7a146b2222484ba88b69306c18bd85",
      "bec42b4bc85c40098868c8bffa95a96b",
      "59acb6d7f252474ca99596a364ec52ed",
      "1cebb7165d1c4d08819ffef6e78b11aa",
      "2fa494e1cbd24ebabf6da738c7cd2be6",
      "71c0f5b068bb422ca02f7f47db8b90c5",
      "df035b8723234e5988f2eb267d73a334",
      "6932fc3441574e50b3408ab9daa42e5e",
      "82089be2c26e4db49d93466346a7e219",
      "402526f9eff84371af39f17304f75637",
      "7a67440671d840d284341d6a124e681e",
      "17c777aafb5f4084b677ad77fb4dfafe",
      "3621b0b861fb471284678b7e22534642",
      "2625a952736e4adc9dc360635abd6ee6",
      "fcf34ebf1f9848718635500393c4a60a",
      "62713bd765604671962f73b29f5dddc2",
      "c9cac29990dd4797a5bb48b69abeaa78",
      "9bd6df79fca34ff08553e06abdc02ac0",
      "91076806376f49dfbfc75a574dd6d5cb",
      "59322913f1274db0a1a68ea255458fed",
      "17b480b3d4814287bbd451049a06c64d",
      "2db4c3074fe44658a86334ac61e6f005",
      "06603c8a77914679a4821ac392ec83cd",
      "c727a345942e41999a3f8eaba670f05b",
      "f09019afa379424685dcd1f93679a25f",
      "32a206eedfbd45769095e464f403db8f",
      "3158fc5509104b3b97b236eddeb368b1",
      "9cac027ca6f84b3eb2d0dd5dc69d4448",
      "5dd8a088938948f78176aa5deffea7cd",
      "dd41d7a40f27478e968d4aa6475592e6",
      "9b5209aadbf241c0a8ec515909a9c1cf",
      "955a4abaa0c14f0da88c7b580fc4fc81",
      "9666c84187254a3e818e5ee34014d729",
      "ee9d65810f5b42f48f9eb69e75e27f30",
      "4d2d9b946df44f2fbcb6483e9964ab17",
      "3e3e29b47a284bc9ae8d3e638276e29b",
      "6dda79081c8b42eb8a260d04b3fc450e",
      "d969beffe78b4301887178a170af7dba",
      "f4d6c9401a3446f0b9c2c158963ee428",
      "fdccdc38c3e247c893e68299de688490",
      "86f8afc7dd6c4386af74174bbdc75362",
      "4e3c817957c44b548946ee5bc4308917",
      "a9f3efd752fd4874988acd4dd13b9239",
      "80f748c3421743a9af21b9f39e5ac503",
      "49081eed50be4d94aac8e89fc0964f75",
      "12b45a942e12462593f4c93f6e88c7c5",
      "7efa8255b7b0434b8614e2e7e1bd832b",
      "c2ee1e7c99a849feb37b2364a0f793fe",
      "31052ca1c0f140f3b161cd87472821fa",
      "d2a5ddb6e7614cbfba69ce73d7abff11",
      "1614338b2f364de69078874ed7571904",
      "51817420f4f7454fa79d8d1e6215565c",
      "0ddc1937e77747c1ab6998b44f43ce9c",
      "2e23f26d256340718dfb5ac045851a53",
      "bcca9b31e7f849319b7d03270f65cf3b",
      "6732b5ccee1247bab579c0a5660aaeb0",
      "bc42d950c25d4dffafda5aa243360f25",
      "93b9c973087b4313818edf9e06b0b125",
      "4d903ab57ec94679902355db40abb706",
      "2cbd961f8cc94b56875ed5a840e34b04",
      "8cb015d7118f43c3bdaac135525cdfb8",
      "d98349926bd44f79ba82053228503d41",
      "7d79704dbd5646e5b5d5adbd4ca05886",
      "e5d7463583f342d59cded3637c4dc898",
      "bfbe2816f3c743e8b6ab6a33bd38e05b",
      "b6d13cf01ecb4b7db8437435d7cc669a",
      "e3be6fc9fd4d40a3a48284fd71158e73",
      "904ecb9b3ed3448189988d213af792dd",
      "f05b6ebda4ef4c5289c272819b998e66",
      "9a66f47d302d40959c3447f5251dff1a",
      "b831e0ae0f0f4d55be90cbdd8efcac05",
      "73950ec447ce4fb1a6245aee5e3993ac",
      "8aa8ca1b4eb941ac87021dc1fbd8046e",
      "28e49db4562d4a108c71e84271e95d3c",
      "16ec2e777dec40efbb472fec7eeba000",
      "6104130d9df1408f8df2b07db9670522",
      "36a0b28199e54b8bae6ed1c814ca4e9a",
      "10f6301d967244c399475b33607c5087",
      "0221df42f513492ab749a4de87afb5d8",
      "c481de34b33c4b839168ca916a83c8bb",
      "e3238650c4414591866844a65808b3fa",
      "dbc387af19e041b4a672f1cad48506f6",
      "085c85ca55fb44969c2535f526493a98",
      "6f56748e6d5549c384dc4eef6ee62afb",
      "a36837b0c1104a63b548ce488e43963b",
      "a64b280a53c543d9b32ee3a9fe69439b",
      "e6f095ab52984ed5af412704676fd374",
      "0bdf15684e4d406d876fb9d97978c8f1",
      "6bcd27ec5f0944e691317e93afed1189",
      "5ad9289cda494c468078d859c73395d4",
      "4e66be05278e4d8a8a3774d43afd9aa6",
      "ee16d1837a694c54abe5d1aa3dd00119",
      "9929cddd8ca24ab3845bf8fa4e24647e",
      "0023f1cbfc984b159556590d9f0b912f",
      "4fa79e6b4ca74853abfead89f79d92e7",
      "9fd0c6b3bc7945d29868d021e265790a",
      "70c8b9025a6e45f5b2f48b199436f0aa",
      "039ff9a36929471f84d853b80306e669",
      "05ce174258b14268884148f250e4047f",
      "3c924c916b4e481ab720cae2fcb893b5",
      "3195bbe7dd8c4db8b5d1b5802707e4e5",
      "12e7731fe86d46a7afd346425448338a",
      "b0c91d5af5264bf591c8ef89ac341426",
      "96efa3345779421c9c30fc8cea242664",
      "71fac7a2c7344a6d9ff8c682cca6ba22",
      "f75a6754d44a4775b67ef995623fdc16",
      "6329801451974fc48c0799c0d04d1623",
      "e1a3c51ab0ed4eb8bbab35fa71efc371",
      "78733758764b44e5ab74685d0c266fc3",
      "58cc7a2c0c3e40a78818404cb5fb6aca",
      "0f8cd90445394477ae95b2518f63c806",
      "4a5683a7362a4ffba39972628862d0a7",
      "9da9762cea64447ba7ccd60574100aa4",
      "f46b1265f28b42ed8abef0e8aa177b20",
      "c3c9a1291048404f9f179801103b838d",
      "c194dae9ab9b4c8bbd078a5a3c940028",
      "c3eec440ab98427cae218955cb755e88",
      "ad6082334d1f45d28b9807221b228cb0",
      "d43c455e70b84cb7b04df6b555fd999c",
      "33ef820cda67404ca7bafb496bdb8d50",
      "ce70bbbff10445f9a9cf67a1ea4dadb2",
      "3ad4e7254bc2425fa1266429bb6665ee",
      "8c853d28cd704a2286306923495870ef",
      "de5fd9e4201c45abaa67abb2d83f03cb",
      "67d2a29e10ba43beb499ab3453ab43c9",
      "d39699e57ffe445b9e7bce579f208dc3",
      "db79bb1b98f746eebb8a1e61cc354c1d",
      "eca193a443ff4193a59737421a8d29af",
      "f041224685d241da987d5c375a26f5d9",
      "ce6e0ab439624796af01fb3d3bfac352",
      "eac989c6956942869716751f6c75f5b3",
      "8abc99fd902b4eb5b59446cca97e073c",
      "3c4ec96ff65d4293b2f7ecb441e1711c",
      "98202f3534f94c8bba76dba7fd7e9209",
      "e839530b0914410b8e93534f283c690a",
      "8767940f43bd423aa8c3dbc5675a296d",
      "83df0e84a37f4210bce7956e44a346bf",
      "065eabac768e48c782b6cb8d81a7d153"
     ]
    },
    "id": "YF3KeYfArLJs",
    "outputId": "c5458655-1ce4-4e79-b681-29948664496e"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load CelebA dataset from Hugging Face\n",
    "dataset = load_dataset(\"flwrlabs/celeba\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG3hKZTKzWMY"
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS4jXZLS1XlU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import toponets\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa3kv-To1d4D"
   },
   "source": [
    "# DataSet+Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "48b0c3bebb5341e680837ce4113579f6",
      "19206568d56f4160b3a4f7e64d90d1f4",
      "0dd6a4f16dda4cf7aa912f4cc348d720",
      "640070fe89254d7a97e7199f12b0e32a",
      "b58b4690b6e644b99b55a8f8b052df67",
      "9ef707bc3eaa489bb99dd142dcd6cf48",
      "f6b180e7a83542b087b14ddfb8e7aebe",
      "b9d23690ce944d8da4d781c40705bead",
      "b917032e72b84ed29217b714b19375e3",
      "70cddf93b175476ab63101ef36ce2e08",
      "20c6d245180d436c842f46ae976933cf",
      "95fa0b54cd934f969c32591d9706cef8",
      "8b7c1c2cc9cb4e2a97d8dac3750cd0da",
      "5d742f59a5ac4051bf42038aa6ef1d5b",
      "5d9c644a38dd4c0e910c4956f7c4803e",
      "c87ca7e709ee4934ab67e34610f31609",
      "289a3c93841f4fca9746f46d1fa0003b",
      "be2eac6033694710aea43c97d43b0caa",
      "0554b392b92640aab9a14dc4a536a93e",
      "2c940481576b450db4aa897ba87bc513",
      "db2f01bcf3c642d286b8c9594acb7c0c",
      "4a85f3f6c80d4e09a9aaed7c41bdddd1",
      "bd997e4d4351456ca17006399c2f6ce1",
      "f85de916fc994121826eb301aeb09c63",
      "4f81169992d64b15b1712119b51a795d",
      "776b176fdb8443a596f9518da9ec31bf",
      "675ace8eb9ce4deca90ebe1f35f6ace5",
      "260c17cc6c2441be94e40341c9b3b9ed",
      "746ac0ca3d8e47819952c7235b23e85b",
      "f06e9545ac344e88820ce7ccde64b40a",
      "457168389ca345eda84f1d1eac9cc879",
      "550111b68e3e471aaca258ee8c7acbf1",
      "c51d351cfed546bf923bf3c377ef8ba5"
     ]
    },
    "id": "eJLRlsIvzFMp",
    "outputId": "f74d85f3-49e2-4dc2-db04-1d37090b8ffd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load CelebA dataset\n",
    "dataset = load_dataset(\"flwrlabs/celeba\")\n",
    "\n",
    "# Define ImageNet preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  #resize for ResNet input\n",
    "    transforms.ToTensor(),  #convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  #imagenet normalization\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpeIqypZ14-8"
   },
   "source": [
    "# Load Pretrained TopoNets ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9e-78mQ14c-",
    "outputId": "008f7f84-66a1-46ad-cc21-6a201aa31582"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained TopoNets ResNet18 model\n",
    "checkpoint_path = \"resnet18_tau_10.pt\"\n",
    "topo_resnet18 = toponets.resnet18(tau=10.0)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "topo_resnet18.load_state_dict(checkpoint)  # Directly load checkpoint if it's a state_dict\n",
    "\n",
    "topo_resnet18.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XHLLlzt1_Pn"
   },
   "source": [
    "# Register a Hook to Capture Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFROHpJJSRUw"
   },
   "outputs": [],
   "source": [
    "class ForwardHook:\n",
    "    def __init__(self, module):\n",
    "        \"\"\"\n",
    "        a nice forward hook\n",
    "        \"\"\"\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "hook = ForwardHook(topo_resnet18.layer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8nokGVi2KQB"
   },
   "source": [
    "# Process Multiple Images & Collect Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UQKG4hmZiue",
    "outputId": "bf493c03-362d-4f28-ffbc-45e5a9d024c4"
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jha0Cs5rZm6n"
   },
   "outputs": [],
   "source": [
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvQhSVdb2MSr",
    "outputId": "d4bba1bc-7559-4a4c-da77-f86b31c85945"
   },
   "outputs": [],
   "source": [
    "# Process multiple images and collect activations\n",
    "num_images = 1000  # Ensure enough samples for PCA\n",
    "activations_list = [] #store activations of each image\n",
    "\n",
    "for i in range(num_images):\n",
    "    image = dataset[\"train\"][i][\"image\"]  #load image\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)  #convert NumPy array to PIL if needed\n",
    "    input_tensor = transform(image).unsqueeze(0)  #Applies preprocessing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = topo_resnet18(input_tensor)  # Run inference\n",
    "        #Feeds the image into the model, triggering the forward hook\n",
    "\n",
    "        mean_along_hw = reduce(\n",
    "        hook.output.numpy(),\n",
    "        \"batch channels height width -> batch channels\",\n",
    "        reduction=\"mean\"\n",
    "    )\n",
    "    #print(mean_along_hw[0][0])\n",
    "\n",
    "    #activations_list.append(activation[\"face_neurons\"].numpy().flatten())\n",
    "    activations_list.append(mean_along_hw.squeeze(0))\n",
    "\n",
    "    #print(activation[\"face_neurons\"].numpy()) # Store activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert activations list to NumPy array\n",
    "flattened_activations = np.array(activations_list)\n",
    "print(flattened_activations)\n",
    "\n",
    "\n",
    "print(\"Activations Shape for PCA:\", flattened_activations.shape)  # num_images, feature_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QKqtiml2S-0"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gBFPwV8wzLg6",
    "outputId": "b7140070-2aac-4c30-d654-1f2a28a08fc0"
   },
   "outputs": [],
   "source": [
    "#Ensure PCA has at least 2 samples\n",
    "#PCA's role: identify the most important neurons in the network that respond to faces\n",
    "#Each neuron in your model has some level of activity when processing faces. Some neurons respond strongly and consistently to face images, while others contribute less significantly.\n",
    "#PCA helps identify which neurons contribute the most to variations in neural activity when the model processes faces\n",
    "\n",
    "num_samples, num_features = flattened_activations.shape\n",
    "if num_samples < 2:\n",
    "    raise ValueError(f\"PCA requires at least 2 samples, but got {num_samples}.\")\n",
    "\n",
    "# Set n_components safely\n",
    "#reduces the dataset to a lower-dimensional representation.\n",
    "\n",
    "n_components = min(512, num_samples, num_features)\n",
    "pca = PCA(n_components=n_components) #create pc model with n comp\n",
    "\n",
    "principal_components = pca.fit_transform(flattened_activations)\n",
    "#Transforms the activations into a lower-dimensional space (300 × n_components).\n",
    "\n",
    "# Print explained variance ratio\n",
    "#explained variance ratio:measure how much important information is kept when simplifying complex data.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Using {n_components} principal components.\")\n",
    "print(\"Explained Variance Ratios:\", pca.explained_variance_ratio_, \"\\n\")\n",
    "cumulative_sum = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(\"Cumulative Explained Variance Ratios:\", cumulative_sum)\n",
    "\n",
    "plt.plot(cumulative_sum)\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Explained Variance vs. Number of Components\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#output\n",
    "#with 20 PCA: PC1 captured 4.86%, PC2 captured 3.16%...\n",
    "#adding up all 20 = 32.9\n",
    "\n",
    "\n",
    "#\tOnly a small portion of the total information is retained-->meaning many neurons contribute to face recognition.\n",
    "# suggests that face recognition is not concentrated in just a few neurons. Instead, many neurons contribute to recognizing faces.\n",
    "\n",
    "#with 50\n",
    "#PC1=4.86, PC2=3.16, PC3=3, PC4=2.9... PC50=0.4\n",
    "#adding up all = 54%\n",
    "\n",
    "#with 100\n",
    "#PC1=4.86...PC100=0.25\n",
    "#adding up all = 71%\n",
    "\n",
    "# still 29% variance remaining-->indicating that face recognition is not dominated by just a few neurons but involves a broad set of neural activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746,
     "referenced_widgets": [
      "9a1a8e7907e745ceb362f632975aaba8",
      "77827350671c4f9f83028d27e7caa539",
      "b2c7b22688114da6bf45af73e2dc7fcf",
      "9468d4a1eca5433b8bce21da81e96221",
      "bb85b9a620b1492f9214a3eed996d336",
      "1ce8a8123aa94ac39b8bc2257d792a08",
      "4ad9ca1815df4555bcfc5c649d061f75",
      "624eb28b896346208226711c44bbe23f",
      "0b76ba1f5f434713a116173e9f678c0b",
      "1c1e427188154c41998bcb66a60a7fb5",
      "71a95d17192045f0a1c73b8d16194fce",
      "5a044b62098144f7a627405269d5e2b8",
      "cd331da5d5524cd1bf7eceb2c456daed",
      "1a267270e60641b7ac3e4eac27a0daa6",
      "538b96bb14fc4d3d9e2906f4d5f56570",
      "5e9dd516988b4aefa828ec3c72fbe980",
      "ff5f8e0fc7f643c399361b882846ac57",
      "b0c2a37cb0fa4c3a91aaa2f3028b487b",
      "6610c8d742334f79ab538073a399cbfc",
      "b3c8137e9234470d8b12b798fa6996b2",
      "c907cfcfade9453f9c5c6af78e514639",
      "261515494d4949338e89a815cb654e62",
      "9628601cae69466ca8bb4ec8e1b149ed",
      "9fcc99653aac487d835f32abdf063ff6",
      "025bdf86b8ce432a91b1e778da16d59b",
      "f6427230942c4cb1a99d450e3dd654f7",
      "9938afa99b7141118278cfbe3d946ca5",
      "7f0e81140b424520a0732f89be157eea",
      "88b51ad0a40e4590a98324b96478e412",
      "70aee30b73aa49699f0292f9b608b91f",
      "16e293e171c04385872fb6b7c5af9709",
      "01be44241c1944f48efb97b6e9703ef3",
      "8d806a08db874408bb828aa0cfdd6dff"
     ]
    },
    "id": "CVgPZGRRcvfR",
    "outputId": "f8c7ee2f-61c8-40d1-daf5-8d57a4d756e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from einops import reduce\n",
    "\n",
    "\n",
    "checkpoint_path = \"resnet18_tau_10.pt\"\n",
    "topo_resnet18 = toponets.resnet18(tau=10.0)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "topo_resnet18.load_state_dict(checkpoint)\n",
    "topo_resnet18.eval()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = load_dataset(\"flwrlabs/celeba\")\n",
    "\n",
    "num_images = 1000  # chnage\n",
    "\n",
    "\n",
    "class ForwardHook:\n",
    "    def __init__(self, module, lesion_channels=None):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.lesion_channels = lesion_channels\n",
    "        self.output = None\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        if self.lesion_channels is not None:\n",
    "            output[:, self.lesion_channels, :, :] = 0\n",
    "        self.output = output.detach()\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "def collect_activations(model, dataset, transform, hook, num_images):\n",
    "    activations = []\n",
    "    for i in range(num_images):\n",
    "        image = dataset[\"train\"][i][\"image\"]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_tensor)\n",
    "            reduced = reduce(\n",
    "                hook.output.numpy(),\n",
    "                \"batch channels height width -> batch channels\",\n",
    "                reduction=\"mean\"\n",
    "            )\n",
    "            activations.append(reduced.squeeze(0))\n",
    "    return np.array(activations)\n",
    "\n",
    "\n",
    "baseline_hook = ForwardHook(topo_resnet18.layer4)\n",
    "baseline_activations = collect_activations(topo_resnet18, dataset, transform, baseline_hook, num_images)\n",
    "baseline_hook.close()\n",
    "\n",
    "\n",
    "face_means = np.mean(baseline_activations, axis=0)\n",
    "threshold = np.percentile(face_means, 95)\n",
    "face_selective_channels = np.where(face_means > threshold)[0]\n",
    "print(\"Face-selective channels:\", face_selective_channels)\n",
    "\n",
    "\n",
    "lesion_hook = ForwardHook(topo_resnet18.layer4, lesion_channels=face_selective_channels)\n",
    "lesioned_activations = collect_activations(topo_resnet18, dataset, transform, lesion_hook, num_images)\n",
    "lesion_hook.close()\n",
    "\n",
    "\n",
    "def run_pca_and_plot(data, label):\n",
    "    pca = PCA(n_components=min(512, *data.shape))\n",
    "    pc = pca.fit_transform(data)\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(cum_var, label=label)\n",
    "    return cum_var\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "run_pca_and_plot(baseline_activations, label=\"Baseline\")\n",
    "run_pca_and_plot(lesioned_activations, label=\"Lesioned (Face-Selective Zeroed)\")\n",
    "plt.xlabel(\"Number of PCA Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA: Baseline vs Lesioned Face-Selective Channels\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AosABUNsjHUj"
   },
   "source": [
    "# Graph Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbxJue2rkRul"
   },
   "source": [
    "-Graph shows that there is a drop in cum explained variance after face-selective neurons were lesioned out\n",
    "-like Kanwisher's fMRI studies, but in a neural network: localizing a “face area” and showing its importance by “virtually lesioning” it, shows how  human brain areas like the Fusiform Face Area (FFA) work, and how damaging that area leads to face perception deficits (prosopagnosia).\n",
    "-zeroing out face-sensitive neurons kinda simulates “brain damage” to the face-recognition part of the network\n",
    "-drop in the orange curve (lesioned) vs. blue (baseline) shows those neurons mattered.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "4n-fN7O8lN_V",
    "outputId": "b37aaf2b-59a1-4385-d0fc-96d7eb64ced1"
   },
   "outputs": [],
   "source": [
    "#try lesioning random neurons and see what happens\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "total_channels = flattened_activations.shape[1]\n",
    "random_channels = np.random.choice(\n",
    "    total_channels,\n",
    "    size=len(face_selective_channels),\n",
    "    replace=False\n",
    ")\n",
    "print(\"Random control channels:\", random_channels)\n",
    "\n",
    "class LesionHook:\n",
    "    def __init__(self, module, lesion_channels):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.lesion_channels = lesion_channels\n",
    "        self.output = None\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        output[:, self.lesion_channels, :, :] = 0  # zero-out selected channels\n",
    "        self.output = output.detach()\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "random_lesion_hook = LesionHook(topo_resnet18.layer4, lesion_channels=random_channels)\n",
    "random_lesioned_activations = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    image = dataset[\"train\"][i][\"image\"]\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = topo_resnet18(input_tensor)\n",
    "        reduced = reduce(\n",
    "            random_lesion_hook.output.numpy(),\n",
    "            \"batch channels height width -> batch channels\",\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "        random_lesioned_activations.append(reduced.squeeze(0))\n",
    "\n",
    "random_lesion_hook.close()\n",
    "random_lesioned_activations = np.array(random_lesioned_activations)\n",
    "\n",
    "\n",
    "def run_pca_and_plot(data, label):\n",
    "    pca = PCA(n_components=min(512, *data.shape))\n",
    "    pc = pca.fit_transform(data)\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(cum_var, label=label)\n",
    "    return cum_var\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "run_pca_and_plot(flattened_activations, label=\"Baseline\")\n",
    "run_pca_and_plot(lesioned_activations, label=\"Face-Selective Lesioned\")\n",
    "run_pca_and_plot(random_lesioned_activations, label=\"Random Neurons Lesioned\")\n",
    "plt.xlabel(\"Number of PCA Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA: Baseline vs Lesioned (Face vs Random)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUCC8v4DmYJV"
   },
   "outputs": [],
   "source": [
    "#image\n",
    "image = dataset[\"train\"][0][\"image\"]\n",
    "if not isinstance(image, Image.Image):\n",
    "    image = Image.fromarray(image)\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "#process through model\n",
    "hook = ForwardHook(topo_resnet18.layer4)\n",
    "_ = topo_resnet18(input_tensor)\n",
    "\n",
    "#visualize 1 face neuron\n",
    "channel_idx = face_selective_channels[0]\n",
    "activation_map = hook.output[0, channel_idx].numpy()\n",
    "\n",
    "plt.imshow(activation_map, cmap='viridis')\n",
    "plt.title(f\"Activation Map – Channel {channel_idx}\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "hook.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHdazE1Zmu8I"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B653f5ol8MHy"
   },
   "source": [
    "# Face vs Scene Image Selectivity Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1591fe3c93db499296acac3528e99f61",
      "0c215f07efa7473a8190f4f829eef8bb",
      "ab78d4b669114618b55cb0cbb9c70098",
      "6dc90869add44fbd88fac3aff6c40863",
      "0f4890ab4c014d21955f800a09d8abcf",
      "9d48712af02049108a6d45d1ff1e858b",
      "5bcc26fef38d4a7da2f5baef32e31f63",
      "14e72064de9e493c9e154b8fb832a1e0",
      "5ce3462f537a4174883fef605c3d3304",
      "3da3e1a7234f44dcb79bde5ebc0afe95",
      "3bf2fa8d281c47b2862833c4ff1246cd",
      "e670d2ce31c842118ee8b86b5510cfb3",
      "55e046565a6e4da0b37f70ae67919552",
      "8130310ad0784208bd2fc09944497074",
      "c5a472e6c22f4469a18a5b28338d821e",
      "da512a1de8594240979e575b17f95ee2",
      "28c79bc9a9da49dcb4b2abdfaeff2812",
      "48d8a5c17e354bcb98ba81e7c2449621",
      "f92a0722406f49c4a6ae48198099699e",
      "6bfa8ef4b6bc4ca9b61c9ce7d11aac0c",
      "043c138cb23349c58429044868743624",
      "95b5f8df07464a2a80696ba0c82bc1cd",
      "ed43deba0d7049ae80cc38a3b5a34877",
      "76d1eaa7d9744395b5c808cac9fb5745",
      "eca00553ef0e46b18f047ee952e9efca",
      "b9ea3d9dd94d4ebeb711899fe661a6df",
      "81e1fe0a6fe1473b895f5d298d66b25f",
      "2c37f325df7b4ab2b7241600ad0312e3",
      "ff9c0e13a1474a8aa322f0cb0ca10f44",
      "c47dea1ac84e4b30a28d161b1442b824",
      "4877fe1464de4403b83c60509f42f19c",
      "32d2e93137664dc6bfb30a77658bd0c8",
      "6fc0baae9f6247e9b6b76cb0482b559a"
     ]
    },
    "id": "iq7MxgSv8Lno",
    "outputId": "4b10683e-bb71-4e8d-d395-4955b7fa6559"
   },
   "outputs": [],
   "source": [
    "# --- Install dependencies ---\n",
    "!pip install git+https://github.com/toponets/toponets.git\n",
    "!pip install datasets einops torchvision matplotlib\n",
    "\n",
    "# --- Imports ---\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import reduce\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Places365\n",
    "from datasets import load_dataset\n",
    "import toponets\n",
    "\n",
    "# --- Set device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load TopoNets pretrained model ---\n",
    "checkpoint_path = \"resnet18_tau_10.pt\"\n",
    "topo_resnet18 = toponets.resnet18(tau=10.0)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "topo_resnet18.load_state_dict(checkpoint)\n",
    "topo_resnet18.to(device)\n",
    "topo_resnet18.eval()\n",
    "\n",
    "# --- Preprocessing (ImageNet style) ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Forward hook for activation capture ---\n",
    "class ForwardHook:\n",
    "    def __init__(self, module, lesion_channels=None):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.lesion_channels = lesion_channels\n",
    "        self.output = None\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        if self.lesion_channels is not None:\n",
    "            output[:, self.lesion_channels, :, :] = 0\n",
    "        self.output = output.detach()\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def collect_activations(model, dataset, transform, hook, num_images):\n",
    "    activations = []\n",
    "    count = 0\n",
    "    for i in range(len(dataset)):\n",
    "        image = dataset[i][\"image\"] if isinstance(dataset[i], dict) else dataset[i][0]\n",
    "\n",
    "        # Convert from tensor to PIL if needed\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_tensor)\n",
    "            reduced = reduce(\n",
    "                hook.output.cpu().numpy(),\n",
    "                \"batch channels height width -> batch channels\",\n",
    "                reduction=\"mean\"\n",
    "            )\n",
    "            activations.append(reduced.squeeze(0))\n",
    "\n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "    return np.array(activations)\n",
    "\n",
    "\n",
    "# --- Load Face Dataset (CelebA) ---\n",
    "face_dataset = load_dataset(\"flwrlabs/celeba\", split=\"train\")\n",
    "\n",
    "# --- Load Scene Dataset (Places365) ---\n",
    "scene_dataset = Places365(root='places365_data',\n",
    "                          split='val',\n",
    "                          small=True,\n",
    "                          download=True,\n",
    "                          transform=transform)\n",
    "\n",
    "# --- Collect activations from face images ---\n",
    "num_images = 1000\n",
    "face_hook = ForwardHook(topo_resnet18.layer4)\n",
    "face_activations = collect_activations(topo_resnet18, face_dataset, transform, face_hook, num_images)\n",
    "face_hook.close()\n",
    "\n",
    "# --- Identify face-selective neurons ---\n",
    "face_means = np.mean(face_activations, axis=0)\n",
    "threshold = np.percentile(face_means, 95)  # Top 5%\n",
    "face_selective_channels = np.where(face_means > threshold)[0]\n",
    "print(f\"Identified {len(face_selective_channels)} face-selective channels.\")\n",
    "\n",
    "# --- Collect activations from scene images ---\n",
    "scene_hook = ForwardHook(topo_resnet18.layer4)\n",
    "scene_activations = collect_activations(topo_resnet18, scene_dataset, transform, scene_hook, num_images)\n",
    "scene_hook.close()\n",
    "\n",
    "# --- Compare activation strengths ---\n",
    "face_selective_on_faces = face_activations[:, face_selective_channels]\n",
    "face_selective_on_scenes = scene_activations[:, face_selective_channels]\n",
    "\n",
    "face_avg = np.mean(face_selective_on_faces)\n",
    "scene_avg = np.mean(face_selective_on_scenes)\n",
    "\n",
    "print(f\"\\nMean activation of face-selective neurons:\")\n",
    "print(f\"- On face images:  {face_avg:.4f}\")\n",
    "print(f\"- On scene images: {scene_avg:.4f}\")\n",
    "\n",
    "# --- PCA Plot ---\n",
    "def run_pca_and_plot(data, label, color):\n",
    "    pca = PCA(n_components=2)\n",
    "    pc = pca.fit_transform(data)\n",
    "    plt.scatter(pc[:, 0], pc[:, 1], label=label, alpha=0.4, s=10, c=color)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "run_pca_and_plot(face_selective_on_faces, \"Face Images\", \"blue\")\n",
    "run_pca_and_plot(face_selective_on_scenes, \"Scene Images\", \"green\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA on Face-Selective Neurons\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dsbv4jha8YVN"
   },
   "source": [
    "# teste face vs scene vs random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMpQzMS9cOoL"
   },
   "source": [
    "We’ll compare the activation distributions of the face-selective neurons when shown:\n",
    "\n",
    "Face images (CelebA)\n",
    "\n",
    "Scene images (Places365)\n",
    "\n",
    "Random object images (CIFAR-10)\n",
    "\n",
    "Goal:\n",
    "If the neurons are really face-selective, their activations should be:\n",
    "\n",
    "High on face images\n",
    "\n",
    "Low on scenes and objects\n",
    "\n",
    "We'll visualize this with a violin plot or box plot, one for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "HAAJ9erb8auK",
    "outputId": "1b0a4a4b-2d96-45aa-b766-5bbac88bb98f"
   },
   "outputs": [],
   "source": [
    "# --- Install CIFAR10 (non-face object images) ---\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "cifar_dataset = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# --- Extract activations on CIFAR (object) images ---\n",
    "cifar_hook = ForwardHook(topo_resnet18.layer4)\n",
    "cifar_activations = collect_activations(topo_resnet18, cifar_dataset, transform, cifar_hook, num_images)\n",
    "cifar_hook.close()\n",
    "\n",
    "# --- Get activations from face-selective neurons ---\n",
    "face_vals = face_activations[:, face_selective_channels].mean(axis=1)\n",
    "scene_vals = scene_activations[:, face_selective_channels].mean(axis=1)\n",
    "cifar_vals = cifar_activations[:, face_selective_channels].mean(axis=1)\n",
    "\n",
    "# --- Plotting ---\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Combine into a DataFrame for seaborn\n",
    "df = pd.DataFrame({\n",
    "    \"Activation\": np.concatenate([face_vals, scene_vals, cifar_vals]),\n",
    "    \"Category\": ([\"Face\"] * len(face_vals)) +\n",
    "                ([\"Scene\"] * len(scene_vals)) +\n",
    "                ([\"Object\"] * len(cifar_vals))\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(data=df, x=\"Category\", y=\"Activation\", palette=\"pastel\")\n",
    "plt.title(\"Mean Activation of Face-Selective Neurons Across Categories\")\n",
    "plt.ylabel(\"Activation Magnitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2qQ3bQieAzq"
   },
   "source": [
    "This boxplot illustrates the average activation magnitude of face-selective neurons in response to three different image categories: Face, Scene, and Object. The neurons exhibit the highest activation for face images, with a median around 1.8 and several data points exceeding 2, indicating strong and consistent responsiveness. In contrast, the activations for scene and object categories are much lower, with medians around 0.3–0.4 and a tighter spread. This suggests that these neurons are specifically tuned to face stimuli and are relatively unresponsive to other types of visual input, reinforcing their selectivity and potential importance in face recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GC06sIvetSR",
    "outputId": "687e605b-2fda-4573-d2fd-93ecbee0798a"
   },
   "outputs": [],
   "source": [
    "!pip install torch-dreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "bN8LaOdwecK9",
    "outputId": "65d9db51-3127-4bf4-be1f-f5d9471cbaf2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_dreams.dreamer import Dreamer\n",
    "\n",
    "# Ensure your topo_resnet18 model is set to evaluation mode\n",
    "model = topo_resnet18\n",
    "model.eval()\n",
    "\n",
    "# Initialize Dreamer with the model and device\n",
    "dreamer = Dreamer(model=model, device=device)\n",
    "\n",
    "# Select a face-selective channel and the corresponding layer\n",
    "target_neuron = face_selective_channels[0]  # Choose the first face-selective neuron\n",
    "target_layer = model.layer4  # The layer where the neuron resides\n",
    "\n",
    "# Define a custom loss function to maximize the activation of the target neuron\n",
    "def custom_loss(layer_outputs):\n",
    "    # layer_outputs is a list; we access the output of the target layer\n",
    "    layer_output = layer_outputs[0]\n",
    "    # Calculate the mean activation of the target neuron\n",
    "    loss = -layer_output[:, target_neuron].mean()\n",
    "    return loss\n",
    "\n",
    "# Generate an image that maximally activates the selected neuron\n",
    "dream_image = dreamer.render(\n",
    "    layers=[target_layer],  # Pass the target layer in a list\n",
    "    custom_func=custom_loss,  # Use the custom loss function\n",
    "    width=224,\n",
    "    height=224,\n",
    "    iters=100,\n",
    "    lr=0.05,\n",
    "    rotate_degrees=15,\n",
    "    scale_max=1.2,\n",
    "    scale_min=0.5,\n",
    "    translate_x=0.2,\n",
    "    translate_y=0.2,\n",
    "    weight_decay=1e-2,\n",
    "    grad_clip=1.0,\n",
    ")\n",
    "\n",
    "# Display the generated image directly using matplotlib\n",
    "plt.imshow(dream_image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Visualization of Neuron {target_neuron}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIrJ5NTYedAW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLbFEW1eecj-"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
